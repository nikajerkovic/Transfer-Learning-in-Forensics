{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Comparison of Transfer Learning Models - Lateral View </h1>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p>\n",
        "Continuing with the model selection process, we proceed to train the three models demonstrating the highest accuracy from the previous step over a span of 10 epochs. It's important to note that, at present, we are not employing any form of data augmentations.</p>"
      ],
      "metadata": {
        "id": "wXLVc9e2jrjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTZgHW5AxbRo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import inspect\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image"
      ],
      "metadata": {
        "id": "MoCvG8lKHXjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCYTR-ZS9ut0",
        "outputId": "2ecef091-28bd-4cce-8d23-c46f43b85136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "data_path = \"/content/drive/MyDrive/sve_LP-bez augumentacije/train\"\n",
        "n_folds =5\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "all_files = glob.glob(os.path.join(data_path, '*/*'))\n",
        "labels = [os.path.basename(os.path.dirname(fp)) for fp in all_files]"
      ],
      "metadata": {
        "id": "NEJNRNoeImRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all available models\n",
        "model_dictionary = {m[0]:m[1] for m in inspect.getmembers(tf.keras.applications, inspect.isfunction)}"
      ],
      "metadata": {
        "id": "XOATCQWS9rQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_dict = {key: model_dictionary[key] for key in model_dictionary  if key in ['Xception', 'MobileNetV2','ResNet50V2']}"
      ],
      "metadata": {
        "id": "OnVkY_hrFn_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptCeZ6bRT24w",
        "outputId": "7e33f1ea-6b62-41b2-be61-9b9ba442c0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_for_fold(train_files, val_files, img_size):\n",
        "    datagen = image.ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "    # Extracting the labels from the directory structure\n",
        "    train_labels = [os.path.basename(os.path.dirname(fp)) for fp in train_files]\n",
        "    val_labels = [os.path.basename(os.path.dirname(fp)) for fp in val_files]\n",
        "\n",
        "    train_gen = datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': train_files, 'class': train_labels}),\n",
        "        directory=None,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        class_mode=\"binary\",\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_gen = datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': val_files, 'class': val_labels}),\n",
        "        directory=None,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        class_mode=\"binary\",\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_gen, val_gen\n"
      ],
      "metadata": {
        "id": "_eAhE1t3HbaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_preprocces(img_size):\n",
        "  test_datagen = image.ImageDataGenerator(rescale=1./255.)\n",
        "  test_generator = test_datagen.flow_from_directory(\"/content/drive/MyDrive/sve_LP-bez augumentacije/test\",\n",
        "                                        target_size=img_size,\n",
        "                                        class_mode='binary',\n",
        "                                        batch_size=batch_size,\n",
        "                                        shuffle=False)\n",
        "  return test_generator"
      ],
      "metadata": {
        "id": "UdiHoYdfJlnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(model_1,input_shape):\n",
        "    pre_trained_model = model_1(include_top=False, pooling='avg', input_shape=input_shape)\n",
        "    pre_trained_model.trainable = False\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(pre_trained_model)\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "P5NTxAvbJVng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over each model available in Keras\n",
        "model_benchmarks = {'model_name': [], 'num_model_params': [], 'validation_accuracy': [], 'test_accuracy': []}\n",
        "for model_name, model_1 in tqdm(list(new_model_dict.items())):\n",
        "    # Special handling for \"NASNetLarge\" since it requires input images with size (331,331)\n",
        "\n",
        "    fold_no = 1\n",
        "    all_acc = []\n",
        "    all_loss = []\n",
        "\n",
        "    for train_indices, val_indices in kfold.split(all_files,labels):\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    # Preprocess data and build model (assuming these functions are defined)\n",
        "        train_files = [all_files[i] for i in train_indices]\n",
        "        val_files = [all_files[i] for i in val_indices]\n",
        "        if 'NASNetLarge' in model_name:\n",
        "          img_size =(331,331)\n",
        "          input_shape =(331,331,3)\n",
        "          train_gen, val_gen = preprocess_data_for_fold(train_files, val_files, img_size)\n",
        "        elif 'InceptionResNetV2' or 'InceptionV3' or 'Xception' in model_name:\n",
        "          img_size=(299,299)\n",
        "          input_shape=(299,299,3)\n",
        "          train_gen, val_gen = preprocess_data_for_fold(train_files, val_files, img_size)\n",
        "        else:\n",
        "          img_size=(224,224)\n",
        "          input_shape=(224,224,3)\n",
        "          train_gen, val_gen = preprocess_data_for_fold(train_files, val_files, img_size)\n",
        "\n",
        "\n",
        "        model = build_model(model_1,input_shape)\n",
        "\n",
        "    # Fit data to model\n",
        "        history = model.fit(train_gen, epochs=10, validation_data=val_gen)\n",
        "\n",
        "    # Evaluate the model on validation set\n",
        "        scores = model.evaluate(val_gen, verbose=0)\n",
        "        print(f'Score for fold {fold_no}: Loss of {scores[0]}; Accuracy of {round(scores[1]*100, 2)}%')\n",
        "        all_acc.append(scores[1] * 100)\n",
        "        all_loss.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "        fold_no += 1\n",
        "\n",
        "# Provide average scores\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print('Score per fold')\n",
        "    for i in range(len(all_acc)):\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print(f'> Fold {i+1} - Loss: {all_loss[i]} - Accuracy: {all_acc[i]}%')\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print('Average scores for all folds:')\n",
        "    print(f'> Accuracy: {np.mean(all_acc)} (+- {np.std(all_acc)})')\n",
        "    print(f'> Loss: {np.mean(all_loss)}')\n",
        "    print('------------------------------------------------------------------------')\n",
        "    if 'NASNetLarge' in model_name:\n",
        "        test_processed = test_preprocces(img_size)\n",
        "    elif 'InceptionResNetV2' or 'InceptionV3' or 'Xception' in model_name:\n",
        "        test_processed = test_preprocces(img_size)\n",
        "    else:\n",
        "        test_processed = test_preprocces(img_size)\n",
        "\n",
        "    test_acc = model.evaluate(test_processed)\n",
        "    predictions = model.predict(test_processed)\n",
        "    predicted_classes = np.where(predictions < 0.5, 0, 1)\n",
        "    true_classes = test_processed.classes\n",
        "    class_labels = list(test_processed.class_indices.keys())\n",
        "\n",
        "    report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "    print(report)\n",
        "\n",
        "    # Calculate all relevant metrics\n",
        "    model_benchmarks['model_name'].append(model_name)\n",
        "    model_benchmarks['num_model_params'].append(model.count_params())\n",
        "    model_benchmarks['validation_accuracy'].append(np.mean(all_acc))\n",
        "    model_benchmarks['test_accuracy'].append(test_acc[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBd5Pr1H5mN",
        "outputId": "afdb465d-ab54-405e-d1a4-70b6cad94466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Found 125 validated image filenames belonging to 2 classes.\n",
            "Found 32 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 34s 6s/step - loss: 0.7641 - accuracy: 0.5120 - val_loss: 0.7025 - val_accuracy: 0.4688\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 439ms/step - loss: 0.6654 - accuracy: 0.5920 - val_loss: 0.7164 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.6821 - accuracy: 0.5040 - val_loss: 0.7026 - val_accuracy: 0.5312\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 540ms/step - loss: 0.6540 - accuracy: 0.6240 - val_loss: 0.6759 - val_accuracy: 0.5312\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 554ms/step - loss: 0.6243 - accuracy: 0.7360 - val_loss: 0.6713 - val_accuracy: 0.5938\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 503ms/step - loss: 0.6157 - accuracy: 0.7200 - val_loss: 0.6595 - val_accuracy: 0.5938\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 443ms/step - loss: 0.6015 - accuracy: 0.8160 - val_loss: 0.6502 - val_accuracy: 0.5938\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 450ms/step - loss: 0.5829 - accuracy: 0.8080 - val_loss: 0.6433 - val_accuracy: 0.5938\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 401ms/step - loss: 0.5707 - accuracy: 0.8240 - val_loss: 0.6353 - val_accuracy: 0.5938\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 441ms/step - loss: 0.5593 - accuracy: 0.8320 - val_loss: 0.6277 - val_accuracy: 0.5938\n",
            "Score for fold 1: Loss of 0.6277283430099487; Accuracy of 59.38%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Found 125 validated image filenames belonging to 2 classes.\n",
            "Found 32 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 749ms/step - loss: 0.7266 - accuracy: 0.4560 - val_loss: 0.7243 - val_accuracy: 0.4062\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 432ms/step - loss: 0.7107 - accuracy: 0.4960 - val_loss: 0.7141 - val_accuracy: 0.4375\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.6790 - accuracy: 0.5360 - val_loss: 0.6868 - val_accuracy: 0.5625\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 643ms/step - loss: 0.6626 - accuracy: 0.6480 - val_loss: 0.6722 - val_accuracy: 0.5938\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 608ms/step - loss: 0.6497 - accuracy: 0.6960 - val_loss: 0.6558 - val_accuracy: 0.7188\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.6338 - accuracy: 0.6880 - val_loss: 0.6409 - val_accuracy: 0.7188\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 433ms/step - loss: 0.6200 - accuracy: 0.7520 - val_loss: 0.6295 - val_accuracy: 0.6875\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 431ms/step - loss: 0.6039 - accuracy: 0.7840 - val_loss: 0.6136 - val_accuracy: 0.7188\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 417ms/step - loss: 0.5893 - accuracy: 0.8000 - val_loss: 0.5996 - val_accuracy: 0.8125\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 425ms/step - loss: 0.5763 - accuracy: 0.8080 - val_loss: 0.5874 - val_accuracy: 0.8125\n",
            "Score for fold 2: Loss of 0.5873610973358154; Accuracy of 81.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 9s 1s/step - loss: 0.6424 - accuracy: 0.6032 - val_loss: 0.6373 - val_accuracy: 0.7419\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 422ms/step - loss: 0.6307 - accuracy: 0.6349 - val_loss: 0.6319 - val_accuracy: 0.6129\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.6135 - accuracy: 0.7381 - val_loss: 0.6165 - val_accuracy: 0.7097\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 490ms/step - loss: 0.5923 - accuracy: 0.7857 - val_loss: 0.6089 - val_accuracy: 0.6774\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 423ms/step - loss: 0.5778 - accuracy: 0.7778 - val_loss: 0.5919 - val_accuracy: 0.8065\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 424ms/step - loss: 0.5638 - accuracy: 0.8016 - val_loss: 0.5810 - val_accuracy: 0.7419\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.5491 - accuracy: 0.8254 - val_loss: 0.5714 - val_accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.5357 - accuracy: 0.8333 - val_loss: 0.5641 - val_accuracy: 0.8387\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 437ms/step - loss: 0.5241 - accuracy: 0.8333 - val_loss: 0.5542 - val_accuracy: 0.8387\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 581ms/step - loss: 0.5127 - accuracy: 0.8571 - val_loss: 0.5466 - val_accuracy: 0.8387\n",
            "Score for fold 3: Loss of 0.546592116355896; Accuracy of 83.87%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 5s 749ms/step - loss: 0.7170 - accuracy: 0.5159 - val_loss: 0.6769 - val_accuracy: 0.5484\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 430ms/step - loss: 0.6827 - accuracy: 0.5238 - val_loss: 0.6810 - val_accuracy: 0.5161\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.6629 - accuracy: 0.5794 - val_loss: 0.6566 - val_accuracy: 0.6129\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 599ms/step - loss: 0.6352 - accuracy: 0.7063 - val_loss: 0.6580 - val_accuracy: 0.5806\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 453ms/step - loss: 0.6260 - accuracy: 0.6667 - val_loss: 0.6469 - val_accuracy: 0.6129\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 441ms/step - loss: 0.6078 - accuracy: 0.7540 - val_loss: 0.6307 - val_accuracy: 0.6774\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 429ms/step - loss: 0.5926 - accuracy: 0.7937 - val_loss: 0.6224 - val_accuracy: 0.7419\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 416ms/step - loss: 0.5779 - accuracy: 0.8016 - val_loss: 0.6152 - val_accuracy: 0.7419\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.5615 - accuracy: 0.8333 - val_loss: 0.6085 - val_accuracy: 0.7097\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.5501 - accuracy: 0.8492 - val_loss: 0.6029 - val_accuracy: 0.7097\n",
            "Score for fold 4: Loss of 0.6028955578804016; Accuracy of 70.97%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 888ms/step - loss: 0.7205 - accuracy: 0.4444 - val_loss: 0.6867 - val_accuracy: 0.5161\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 411ms/step - loss: 0.6991 - accuracy: 0.5317 - val_loss: 0.6609 - val_accuracy: 0.6129\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6702 - accuracy: 0.6190 - val_loss: 0.6462 - val_accuracy: 0.6452\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 544ms/step - loss: 0.6560 - accuracy: 0.6746 - val_loss: 0.6321 - val_accuracy: 0.8065\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 511ms/step - loss: 0.6400 - accuracy: 0.7143 - val_loss: 0.6216 - val_accuracy: 0.8387\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 435ms/step - loss: 0.6235 - accuracy: 0.7540 - val_loss: 0.6112 - val_accuracy: 0.8387\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.6089 - accuracy: 0.8016 - val_loss: 0.6030 - val_accuracy: 0.7742\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 423ms/step - loss: 0.5942 - accuracy: 0.7937 - val_loss: 0.5912 - val_accuracy: 0.8065\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 414ms/step - loss: 0.5809 - accuracy: 0.8016 - val_loss: 0.5810 - val_accuracy: 0.8065\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 409ms/step - loss: 0.5671 - accuracy: 0.8413 - val_loss: 0.5725 - val_accuracy: 0.8387\n",
            "Score for fold 5: Loss of 0.5725099444389343; Accuracy of 83.87%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6277283430099487 - Accuracy: 59.375%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5873610973358154 - Accuracy: 81.25%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.546592116355896 - Accuracy: 83.87096524238586%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6028955578804016 - Accuracy: 70.96773982048035%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5725099444389343 - Accuracy: 83.87096524238586%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 75.86693406105042 (+- 9.519374165222514)\n",
            "> Loss: 0.5874174118041993\n",
            "------------------------------------------------------------------------\n",
            "Found 67 images belonging to 2 classes.\n",
            "3/3 [==============================] - 10s 5s/step - loss: 0.5985 - accuracy: 0.6866\n",
            "3/3 [==============================] - 1s 126ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [03:20<06:40, 200.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           F       0.71      0.61      0.66        33\n",
            "           M       0.67      0.76      0.71        34\n",
            "\n",
            "    accuracy                           0.69        67\n",
            "   macro avg       0.69      0.69      0.68        67\n",
            "weighted avg       0.69      0.69      0.68        67\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Found 125 validated image filenames belonging to 2 classes.\n",
            "Found 32 validated image filenames belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94668760/94668760 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 14s 2s/step - loss: 0.7465 - accuracy: 0.4560 - val_loss: 0.7454 - val_accuracy: 0.5312\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 444ms/step - loss: 0.7161 - accuracy: 0.4880 - val_loss: 0.6839 - val_accuracy: 0.5625\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 547ms/step - loss: 0.6715 - accuracy: 0.6000 - val_loss: 0.6674 - val_accuracy: 0.6250\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 563ms/step - loss: 0.6610 - accuracy: 0.6080 - val_loss: 0.6421 - val_accuracy: 0.6875\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 456ms/step - loss: 0.6221 - accuracy: 0.6960 - val_loss: 0.6428 - val_accuracy: 0.5938\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 464ms/step - loss: 0.6009 - accuracy: 0.6720 - val_loss: 0.6294 - val_accuracy: 0.6250\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 448ms/step - loss: 0.5770 - accuracy: 0.7200 - val_loss: 0.5968 - val_accuracy: 0.6562\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 440ms/step - loss: 0.5556 - accuracy: 0.7760 - val_loss: 0.5818 - val_accuracy: 0.7812\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 470ms/step - loss: 0.5406 - accuracy: 0.8000 - val_loss: 0.5727 - val_accuracy: 0.7188\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 622ms/step - loss: 0.5238 - accuracy: 0.8160 - val_loss: 0.5731 - val_accuracy: 0.6562\n",
            "Score for fold 1: Loss of 0.573096513748169; Accuracy of 65.62%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Found 125 validated image filenames belonging to 2 classes.\n",
            "Found 32 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 796ms/step - loss: 0.6291 - accuracy: 0.6560 - val_loss: 0.5997 - val_accuracy: 0.6250\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 440ms/step - loss: 0.6122 - accuracy: 0.6880 - val_loss: 0.5679 - val_accuracy: 0.8438\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 566ms/step - loss: 0.5737 - accuracy: 0.7520 - val_loss: 0.5552 - val_accuracy: 0.7812\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 613ms/step - loss: 0.5573 - accuracy: 0.7440 - val_loss: 0.5402 - val_accuracy: 0.7812\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 438ms/step - loss: 0.5368 - accuracy: 0.7760 - val_loss: 0.5241 - val_accuracy: 0.7812\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.5202 - accuracy: 0.8080 - val_loss: 0.5146 - val_accuracy: 0.7812\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 453ms/step - loss: 0.5035 - accuracy: 0.8000 - val_loss: 0.5072 - val_accuracy: 0.7812\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 472ms/step - loss: 0.4903 - accuracy: 0.8000 - val_loss: 0.4998 - val_accuracy: 0.7812\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 457ms/step - loss: 0.4774 - accuracy: 0.8080 - val_loss: 0.4913 - val_accuracy: 0.7812\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 567ms/step - loss: 0.4657 - accuracy: 0.8160 - val_loss: 0.4855 - val_accuracy: 0.7812\n",
            "Score for fold 2: Loss of 0.48553910851478577; Accuracy of 78.12%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.7386 - accuracy: 0.4365 - val_loss: 0.6995 - val_accuracy: 0.4839\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.7060 - accuracy: 0.4841 - val_loss: 0.6739 - val_accuracy: 0.6774\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 455ms/step - loss: 0.6535 - accuracy: 0.6349 - val_loss: 0.6601 - val_accuracy: 0.5806\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 450ms/step - loss: 0.6219 - accuracy: 0.7222 - val_loss: 0.6519 - val_accuracy: 0.5806\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 445ms/step - loss: 0.5889 - accuracy: 0.7698 - val_loss: 0.6382 - val_accuracy: 0.5484\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 505ms/step - loss: 0.5618 - accuracy: 0.8016 - val_loss: 0.6296 - val_accuracy: 0.5806\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 600ms/step - loss: 0.5365 - accuracy: 0.8413 - val_loss: 0.6224 - val_accuracy: 0.6129\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.5164 - accuracy: 0.8016 - val_loss: 0.6208 - val_accuracy: 0.6129\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 457ms/step - loss: 0.4964 - accuracy: 0.8333 - val_loss: 0.6140 - val_accuracy: 0.6129\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 412ms/step - loss: 0.4852 - accuracy: 0.8492 - val_loss: 0.6060 - val_accuracy: 0.6452\n",
            "Score for fold 3: Loss of 0.6060173511505127; Accuracy of 64.52%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 7s 895ms/step - loss: 0.6981 - accuracy: 0.5159 - val_loss: 0.7227 - val_accuracy: 0.5161\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 456ms/step - loss: 0.6694 - accuracy: 0.5635 - val_loss: 0.6556 - val_accuracy: 0.6129\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 442ms/step - loss: 0.6223 - accuracy: 0.6746 - val_loss: 0.6439 - val_accuracy: 0.6452\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 455ms/step - loss: 0.5952 - accuracy: 0.7381 - val_loss: 0.6329 - val_accuracy: 0.6452\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 421ms/step - loss: 0.5739 - accuracy: 0.7540 - val_loss: 0.6310 - val_accuracy: 0.7097\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 474ms/step - loss: 0.5563 - accuracy: 0.7778 - val_loss: 0.6146 - val_accuracy: 0.6774\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 620ms/step - loss: 0.5372 - accuracy: 0.7778 - val_loss: 0.6026 - val_accuracy: 0.7419\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 518ms/step - loss: 0.5190 - accuracy: 0.8254 - val_loss: 0.5968 - val_accuracy: 0.7742\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 464ms/step - loss: 0.5034 - accuracy: 0.8175 - val_loss: 0.5903 - val_accuracy: 0.7742\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 476ms/step - loss: 0.4883 - accuracy: 0.8175 - val_loss: 0.5831 - val_accuracy: 0.7419\n",
            "Score for fold 4: Loss of 0.5831050276756287; Accuracy of 74.19%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 7s 1s/step - loss: 0.7175 - accuracy: 0.4762 - val_loss: 0.7149 - val_accuracy: 0.4516\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6884 - accuracy: 0.5397 - val_loss: 0.7078 - val_accuracy: 0.5161\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 459ms/step - loss: 0.6573 - accuracy: 0.5873 - val_loss: 0.6595 - val_accuracy: 0.5806\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 458ms/step - loss: 0.6273 - accuracy: 0.6587 - val_loss: 0.6396 - val_accuracy: 0.6129\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 450ms/step - loss: 0.6046 - accuracy: 0.7460 - val_loss: 0.6244 - val_accuracy: 0.6129\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 449ms/step - loss: 0.5820 - accuracy: 0.7540 - val_loss: 0.6028 - val_accuracy: 0.6774\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 596ms/step - loss: 0.5642 - accuracy: 0.7698 - val_loss: 0.5928 - val_accuracy: 0.7097\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 544ms/step - loss: 0.5495 - accuracy: 0.7619 - val_loss: 0.5755 - val_accuracy: 0.6774\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 442ms/step - loss: 0.5285 - accuracy: 0.7937 - val_loss: 0.5673 - val_accuracy: 0.7419\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 461ms/step - loss: 0.5127 - accuracy: 0.7857 - val_loss: 0.5620 - val_accuracy: 0.7742\n",
            "Score for fold 5: Loss of 0.5620027184486389; Accuracy of 77.42%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.573096513748169 - Accuracy: 65.625%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.48553910851478577 - Accuracy: 78.125%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6060173511505127 - Accuracy: 64.51612710952759%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5831050276756287 - Accuracy: 74.19354915618896%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5620027184486389 - Accuracy: 77.4193525314331%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 71.97580575942993 (+- 5.802460105764309)\n",
            "> Loss: 0.561952143907547\n",
            "------------------------------------------------------------------------\n",
            "Found 67 images belonging to 2 classes.\n",
            "3/3 [==============================] - 2s 721ms/step - loss: 0.5817 - accuracy: 0.7015\n",
            "3/3 [==============================] - 2s 154ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [06:03<02:58, 178.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           F       0.68      0.76      0.71        33\n",
            "           M       0.73      0.65      0.69        34\n",
            "\n",
            "    accuracy                           0.70        67\n",
            "   macro avg       0.70      0.70      0.70        67\n",
            "weighted avg       0.70      0.70      0.70        67\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Found 125 validated image filenames belonging to 2 classes.\n",
            "Found 32 validated image filenames belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 17s 2s/step - loss: 0.6801 - accuracy: 0.6000 - val_loss: 0.6779 - val_accuracy: 0.6875\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 482ms/step - loss: 0.6575 - accuracy: 0.6640 - val_loss: 0.6655 - val_accuracy: 0.6875\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 513ms/step - loss: 0.6361 - accuracy: 0.7040 - val_loss: 0.6534 - val_accuracy: 0.6562\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 510ms/step - loss: 0.6185 - accuracy: 0.7360 - val_loss: 0.6439 - val_accuracy: 0.6875\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 476ms/step - loss: 0.6053 - accuracy: 0.7200 - val_loss: 0.6377 - val_accuracy: 0.6562\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.5891 - accuracy: 0.7440 - val_loss: 0.6305 - val_accuracy: 0.7500\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 613ms/step - loss: 0.5740 - accuracy: 0.7840 - val_loss: 0.6251 - val_accuracy: 0.7500\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 627ms/step - loss: 0.5655 - accuracy: 0.7280 - val_loss: 0.6222 - val_accuracy: 0.6875\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.5553 - accuracy: 0.7520 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 513ms/step - loss: 0.5429 - accuracy: 0.7680 - val_loss: 0.6128 - val_accuracy: 0.7188\n",
            "Score for fold 1: Loss of 0.6128391027450562; Accuracy of 71.88%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Found 125 validated image filenames belonging to 2 classes.\n",
            "Found 32 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.7001 - accuracy: 0.4240 - val_loss: 0.6868 - val_accuracy: 0.6250\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 501ms/step - loss: 0.6761 - accuracy: 0.6640 - val_loss: 0.6620 - val_accuracy: 0.6562\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 480ms/step - loss: 0.6597 - accuracy: 0.7120 - val_loss: 0.6389 - val_accuracy: 0.7188\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 478ms/step - loss: 0.6437 - accuracy: 0.7600 - val_loss: 0.6188 - val_accuracy: 0.7500\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 506ms/step - loss: 0.6297 - accuracy: 0.7760 - val_loss: 0.5996 - val_accuracy: 0.7812\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 495ms/step - loss: 0.6157 - accuracy: 0.7840 - val_loss: 0.5848 - val_accuracy: 0.7188\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 596ms/step - loss: 0.6048 - accuracy: 0.7520 - val_loss: 0.5718 - val_accuracy: 0.7188\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 547ms/step - loss: 0.5940 - accuracy: 0.7760 - val_loss: 0.5575 - val_accuracy: 0.7812\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 478ms/step - loss: 0.5859 - accuracy: 0.7840 - val_loss: 0.5469 - val_accuracy: 0.7812\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 514ms/step - loss: 0.5742 - accuracy: 0.8000 - val_loss: 0.5354 - val_accuracy: 0.7500\n",
            "Score for fold 2: Loss of 0.5354371070861816; Accuracy of 75.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 16s 4s/step - loss: 0.6929 - accuracy: 0.5159 - val_loss: 0.6786 - val_accuracy: 0.6452\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 480ms/step - loss: 0.6692 - accuracy: 0.6032 - val_loss: 0.6690 - val_accuracy: 0.5806\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 462ms/step - loss: 0.6499 - accuracy: 0.6746 - val_loss: 0.6544 - val_accuracy: 0.6774\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 464ms/step - loss: 0.6322 - accuracy: 0.7460 - val_loss: 0.6431 - val_accuracy: 0.6129\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 571ms/step - loss: 0.6162 - accuracy: 0.7698 - val_loss: 0.6350 - val_accuracy: 0.6774\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 595ms/step - loss: 0.6019 - accuracy: 0.7778 - val_loss: 0.6298 - val_accuracy: 0.7419\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 474ms/step - loss: 0.5884 - accuracy: 0.7937 - val_loss: 0.6232 - val_accuracy: 0.6452\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 493ms/step - loss: 0.5768 - accuracy: 0.7937 - val_loss: 0.6175 - val_accuracy: 0.6452\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 507ms/step - loss: 0.5656 - accuracy: 0.7857 - val_loss: 0.6137 - val_accuracy: 0.7419\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 476ms/step - loss: 0.5566 - accuracy: 0.7857 - val_loss: 0.6081 - val_accuracy: 0.6774\n",
            "Score for fold 3: Loss of 0.6081234216690063; Accuracy of 67.74%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 7s 773ms/step - loss: 0.6903 - accuracy: 0.5556 - val_loss: 0.6923 - val_accuracy: 0.5484\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 498ms/step - loss: 0.6651 - accuracy: 0.6349 - val_loss: 0.6833 - val_accuracy: 0.5484\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 489ms/step - loss: 0.6469 - accuracy: 0.6587 - val_loss: 0.6733 - val_accuracy: 0.6129\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 595ms/step - loss: 0.6316 - accuracy: 0.7381 - val_loss: 0.6624 - val_accuracy: 0.7097\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 626ms/step - loss: 0.6150 - accuracy: 0.7540 - val_loss: 0.6547 - val_accuracy: 0.6774\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 508ms/step - loss: 0.6008 - accuracy: 0.7698 - val_loss: 0.6494 - val_accuracy: 0.6774\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.5897 - accuracy: 0.7540 - val_loss: 0.6452 - val_accuracy: 0.6452\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 481ms/step - loss: 0.5760 - accuracy: 0.7857 - val_loss: 0.6403 - val_accuracy: 0.6452\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 477ms/step - loss: 0.5646 - accuracy: 0.7937 - val_loss: 0.6347 - val_accuracy: 0.6774\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.5537 - accuracy: 0.7857 - val_loss: 0.6298 - val_accuracy: 0.6452\n",
            "Score for fold 4: Loss of 0.6298388838768005; Accuracy of 64.52%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Found 126 validated image filenames belonging to 2 classes.\n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 817ms/step - loss: 0.7077 - accuracy: 0.4206 - val_loss: 0.6855 - val_accuracy: 0.6129\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 488ms/step - loss: 0.6834 - accuracy: 0.6111 - val_loss: 0.6786 - val_accuracy: 0.5806\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 540ms/step - loss: 0.6624 - accuracy: 0.6667 - val_loss: 0.6648 - val_accuracy: 0.7419\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 631ms/step - loss: 0.6449 - accuracy: 0.7222 - val_loss: 0.6537 - val_accuracy: 0.7742\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 688ms/step - loss: 0.6327 - accuracy: 0.7540 - val_loss: 0.6423 - val_accuracy: 0.7097\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 468ms/step - loss: 0.6166 - accuracy: 0.7698 - val_loss: 0.6349 - val_accuracy: 0.7419\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6054 - accuracy: 0.7698 - val_loss: 0.6301 - val_accuracy: 0.7742\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 488ms/step - loss: 0.5921 - accuracy: 0.7857 - val_loss: 0.6240 - val_accuracy: 0.7742\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.5812 - accuracy: 0.7857 - val_loss: 0.6182 - val_accuracy: 0.7742\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 625ms/step - loss: 0.5716 - accuracy: 0.7937 - val_loss: 0.6132 - val_accuracy: 0.7742\n",
            "Score for fold 5: Loss of 0.6131803393363953; Accuracy of 77.42%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6128391027450562 - Accuracy: 71.875%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5354371070861816 - Accuracy: 75.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6081234216690063 - Accuracy: 67.7419364452362%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6298388838768005 - Accuracy: 64.51612710952759%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6131803393363953 - Accuracy: 77.4193525314331%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 71.31048321723938 (+- 4.693555125244897)\n",
            "> Loss: 0.599883770942688\n",
            "------------------------------------------------------------------------\n",
            "Found 67 images belonging to 2 classes.\n",
            "3/3 [==============================] - 2s 957ms/step - loss: 0.6427 - accuracy: 0.6716\n",
            "3/3 [==============================] - 1s 207ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [09:05<00:00, 181.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           F       0.67      0.67      0.67        33\n",
            "           M       0.68      0.68      0.68        34\n",
            "\n",
            "    accuracy                           0.67        67\n",
            "   macro avg       0.67      0.67      0.67        67\n",
            "weighted avg       0.67      0.67      0.67        67\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Results to DataFrame for easy viewing\n",
        "benchmark_df = pd.DataFrame(model_benchmarks)\n",
        "benchmark_df.sort_values('num_model_params', inplace=True) # sort in ascending order of num_model_params column\n",
        "benchmark_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "KYvHlti8Kjb1",
        "outputId": "e2f0c680-59b0-441b-da02-cd761aa13597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    model_name  num_model_params  validation_accuracy  test_accuracy\n",
              "0  MobileNetV2           2259265            75.866934       0.686567\n",
              "2     Xception          20863529            71.310483       0.671642\n",
              "1   ResNet50V2          23566849            71.975806       0.701493"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f83e18a8-41ee-4b31-affb-de4187614b06\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>test_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2259265</td>\n",
              "      <td>75.866934</td>\n",
              "      <td>0.686567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20863529</td>\n",
              "      <td>71.310483</td>\n",
              "      <td>0.671642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23566849</td>\n",
              "      <td>71.975806</td>\n",
              "      <td>0.701493</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f83e18a8-41ee-4b31-affb-de4187614b06')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f83e18a8-41ee-4b31-affb-de4187614b06 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f83e18a8-41ee-4b31-affb-de4187614b06');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f27beb1d-56dd-4e72-b5bc-d17deef6df3b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f27beb1d-56dd-4e72-b5bc-d17deef6df3b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f27beb1d-56dd-4e72-b5bc-d17deef6df3b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}